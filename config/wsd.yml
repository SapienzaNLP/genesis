paths:
  output_folder:
    checkpoints # path to the output folder where the checkpoints will be saved during training. If the path does not exists, the folder will be created.

  data_dir:
    data/ # path to the data directory.

  scorer_dir:
    scoring_scripts/ # path to the scoring scripts directory.

exp_name:
  bart # name of the experiment, will be used as prefix for the checkpoint folder. If it contains 'debug', logs won't be saved.

datasets:
  wsd_dataset: adv-only # dev set name.
  
model:
  name: facebook/bart-large # name for HuggingFace method .from_pretrained()
  max_tokens_per_batch: 1600
  dropout: 0.1
  decoder_layerdropout: 0.2
  encoder_layerdropout: 0.6
  seed: 313

optimiser:
  learning_rate: 0.00001
  weight_decay: 0.01
  no_decay_params:
    - "bias"
    - "LayerNorm.weight"

generation_parameters:
  num_beams: 15 # beam size --> at training time, they will be only used to define a folder where the checkpoints will be saved (see README.md)
  num_return_sequences: 3 # returned sequences from beam search
  early_stopping: True # early stopping in generation

wandb:
  project_name: genesis

trainer:

  checkpoint:
    filename: '{epoch}-{val_prec:.2f}-{val_accuracy:.3f}' # format for the checkpoint file name
    save_top_k: 1 # save best performing epoch on dev set (according to accuracy)
    save_last: False
    monitor: val_accuracy
    mode: max

  patience: 2
  max_epochs: 100
  use_amp: True

  gradient:
    accumulation: 2
    clipping: 5.0

shorten_gen_keys: # mapping from generation parameters that are passed to the .generate() method and strings used to create the output folder
    num_beams: beams
    num_return_sequences: return
    early_stopping: None
    length_penalty: len_penalty
    min_length: min_len
    do_sample: sample
    top_k: top_k
    temperature: temp
    no_repeat_ngram_size: no_rep


